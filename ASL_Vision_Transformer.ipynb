{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyllanesl/AI-EDGE-Project/blob/main/ClassifierTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download Appropriate Packages"
      ],
      "metadata": {
        "id": "7jnSB2hL8R_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow pillow\n",
        "!pip install mediapipe\n",
        "!pip install transformers datasets torch torchvision accelerate optuna -U\n",
        "!pip install huggingface_hub\n",
        "!pip install pandas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mzBqHD6Bbh_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load dataset"
      ],
      "metadata": {
        "id": "570jXKiNpmNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "dataset = load_dataset('raulit04/ASL_Dataset1')"
      ],
      "metadata": {
        "id": "3D7vCkxEprU4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Preprocessing Functions\n",
        "\n"
      ],
      "metadata": {
        "id": "gFkqh6nGOp28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Load the image processor\n",
        "image_processor = ViTImageProcessor.from_pretrained('dyllanesl/ASL_Classifier')\n",
        "\n",
        "# Define data augmentation transformations (without normalization)\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    # Normalization will be handled by the image processor\n",
        "])\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    # Convert to RGB before applying transforms\n",
        "    images = [data_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
        "    # Convert the list of tensors to a single tensor for batch processing\n",
        "    pixel_values = torch.stack(images)\n",
        "    # Pass the preprocessed images to the image processor\n",
        "    inputs = image_processor(images=pixel_values, return_tensors=\"pt\", do_rescale=False)\n",
        "    inputs['label'] = examples['label']\n",
        "    return inputs\n",
        "\n",
        "# Apply the preprocessing function\n",
        "dataset = dataset.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "id": "pG67a-SOguID",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load pretrained model"
      ],
      "metadata": {
        "id": "mDJqa6LgkTtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTForImageClassification\n",
        "\n",
        "# Define the number of classes in your dataset\n",
        "num_labels = len(dataset['train'].unique('label'))\n",
        "\n",
        "# Load the pre-trained model with the correct number of output labels\n",
        "model = ViTForImageClassification.from_pretrained('dyllanesl/ASL_Classifier', num_labels=num_labels,ignore_mismatched_sizes=True, low_cpu_mem_usage=False)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zzflDirIkEyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define training arguments and trainer"
      ],
      "metadata": {
        "id": "0Q1m64xMkXWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_scheduler, EarlyStoppingCallback\n",
        "import optuna\n",
        "\n",
        "# Create a mapping from labels to IDs\n",
        "label_list = dataset['train'].unique('label')  # Get unique labels\n",
        "label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
        "\n",
        "# Define data collator\n",
        "def data_collator(features):\n",
        "    pixel_values = torch.stack([torch.tensor(f['pixel_values']) for f in features])  # Convert to tensors before stacking\n",
        "    labels = torch.tensor([label_to_id[f['label']] for f in features])  # Map labels to integers\n",
        "    return {'pixel_values': pixel_values, 'labels': labels}\n",
        "\n",
        "# Split the dataset into train and validation\n",
        "train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "# Load the pre-trained model with the correct number of output labels\n",
        "num_labels = len(dataset['train'].unique('label'))\n",
        "model = ViTForImageClassification.from_pretrained('dyllanesl/ASL_Classifier', num_labels=num_labels, ignore_mismatched_sizes=True, low_cpu_mem_usage=False)\n",
        "\n",
        "# Optuna objective function for hyperparameter tuning\n",
        "def objective(trial):\n",
        "    # Suggest hyperparameters\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
        "    num_train_epochs = trial.suggest_int('num_train_epochs', 3, 8)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./vision_transformer_model_progress',\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        evaluation_strategy='steps',  # Evaluate the model at each save step\n",
        "        save_strategy='steps',        # Save checkpoints at regular intervals\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        learning_rate=learning_rate,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        save_steps=200,  # Save model every 100 steps\n",
        "        eval_steps=200,  # Evaluate model every 100 steps\n",
        "        load_best_model_at_end=True, #loads best version\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "        fp16=True  # Enable mixed precision training\n",
        "    )\n",
        "\n",
        "    # Define learning rate scheduler\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    lr_scheduler = get_scheduler(\n",
        "        name=\"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=500,\n",
        "        num_training_steps=len(train_dataset) * num_train_epochs,\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        data_collator=data_collator,\n",
        "        optimizers=(optimizer, lr_scheduler),\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    eval_result = trainer.evaluate()\n",
        "\n",
        "    # Save the final model\n",
        "    trainer.save_model(\"./vision_transformer_model_progress\")\n",
        "\n",
        "    return eval_result['eval_loss']\n",
        "\n",
        "# Run Optuna optimization\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# Print the best trial\n",
        "print(f\"Best trial: {study.best_trial.number}\")\n",
        "print(f\"Best trial value (eval_loss): {study.best_trial.value}\")\n",
        "print(f\"Best hyperparameters: {study.best_trial.params}\")\n"
      ],
      "metadata": {
        "id": "m-qnHkMFkavW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save Model (if results are improved)"
      ],
      "metadata": {
        "id": "caE1e3XxUPZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Model\n",
        "NOTE: During actual employment of project, make sure to integrate image taking in a loop"
      ],
      "metadata": {
        "id": "jE4jpO82T8G9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "            const div = document.createElement('div');\n",
        "            const capture = document.createElement('button');\n",
        "            capture.textContent = 'Capture';\n",
        "            div.appendChild(capture);\n",
        "\n",
        "            const video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "            document.body.appendChild(div);\n",
        "            div.appendChild(video);\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "\n",
        "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "            await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            div.remove();\n",
        "            return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "# Capture photo\n",
        "photo_filename = take_photo()"
      ],
      "metadata": {
        "id": "NN9XehcsDPiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "MARGIN = 10 # pixels\n",
        "FONT_SIZE = 1\n",
        "FONT_THICKNESS = 1\n",
        "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # Define HANDEDNESS_TEXT_COLOR\n",
        "\n",
        "# Function to draw landmarks on an image\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "    hand_landmarks_list = detection_result.multi_hand_landmarks\n",
        "    handedness_list = detection_result.multi_handedness\n",
        "    annotated_image = np.copy(rgb_image)\n",
        "\n",
        "    for idx in range(len(hand_landmarks_list)):\n",
        "        hand_landmarks = hand_landmarks_list[idx]\n",
        "        handedness = handedness_list[idx]\n",
        "\n",
        "        mp.solutions.drawing_utils.draw_landmarks(\n",
        "            annotated_image,\n",
        "            hand_landmarks,\n",
        "            mp.solutions.hands.HAND_CONNECTIONS,\n",
        "            mp.solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
        "            mp.solutions.drawing_styles.get_default_hand_connections_style()\n",
        "        )\n",
        "\n",
        "        height, width, _ = annotated_image.shape\n",
        "        x_coordinates = [landmark.x for landmark in hand_landmarks.landmark]\n",
        "        y_coordinates = [landmark.y for landmark in hand_landmarks.landmark]\n",
        "        text_x = int(min(x_coordinates) * width)\n",
        "        text_y = int(min(y_coordinates) * height) - MARGIN\n",
        "\n",
        "        cv2.putText(annotated_image, f\"{handedness.classification[0].label}\",\n",
        "                    (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
        "                    FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
        "\n",
        "    return annotated_image\n",
        "\n",
        "# Initialize MediaPipe Hands\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n",
        "\n",
        "# Load the captured image\n",
        "image = cv2.imread(photo_filename)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Process the image to detect hand landmarks\n",
        "results = hands.process(image_rgb)\n",
        "\n",
        "# Draw landmarks on the image\n",
        "if results.multi_hand_landmarks:\n",
        "    annotated_image = draw_landmarks_on_image(image_rgb, results)\n",
        "    annotated_image_bgr = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
        "    cv2.imwrite('annotated_photo.jpg', annotated_image_bgr)\n",
        "else:\n",
        "    annotated_image_bgr = image\n",
        "    print(\"No hand landmarks detected.\")\n"
      ],
      "metadata": {
        "id": "L9A5HvEvDP43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "import torch\n",
        "\n",
        "# Load the feature extractor and model\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "model = ViTForImageClassification.from_pretrained('./vision_transformer_model_progress/checkpoint-198')\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "RDVWFMSIWfPm",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1a86f98-68fc-468b-ce1d-3a07d5d12042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTForImageClassification(\n",
              "  (vit): ViTModel(\n",
              "    (embeddings): ViTEmbeddings(\n",
              "      (patch_embeddings): ViTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViTLayer(\n",
              "          (attention): ViTSdpaAttention(\n",
              "            (attention): ViTSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViTSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViTIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViTOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=26, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Load and preprocess the image\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "    return inputs['pixel_values']\n",
        "\n",
        "image_path = '/content/annotated_photo.jpg'  # Replace with the path to your image\n",
        "pixel_values = preprocess_image(image_path)"
      ],
      "metadata": {
        "id": "exxXXJNjWjkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(pixel_values)\n",
        "\n",
        "# Get predicted class label\n",
        "preds = outputs.logits.argmax(-1).item()\n",
        "\n",
        "# Map the prediction to the label\n",
        "id_to_label = {v: k for k, v in label_to_id.items()}  # Assuming label_to_id is defined as in your training script\n",
        "predicted_label = id_to_label[preds]\n",
        "\n",
        "# Ensure predicted_label is a string\n",
        "predicted_label_str = str(predicted_label)\n",
        "\n",
        "print(f'Predicted label: {predicted_label_str}')"
      ],
      "metadata": {
        "id": "7taUsHSDWnix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load CSV File and Create a Custom Dataset"
      ],
      "metadata": {
        "id": "AbCmGwNuO3aQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save Model Progress"
      ],
      "metadata": {
        "id": "MCRQGdGTbbrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "LUNunUhPbetD",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "login(token = userdata.get('ASL_Token'))"
      ],
      "metadata": {
        "id": "N4YWXGHGrXYk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, create_repo\n",
        "import os\n",
        "\n",
        "api = HfApi()\n",
        "repo_id = \"dyllanesl/ASL_Classifier\"  # Your repository name\n",
        "token = userdata.get('ASL_Token')  # Your Hugging Face token\n",
        "\n",
        "# Create the repository if it doesn't exist\n",
        "try:\n",
        "    temp = api.repo_info(repo_id, repo_type=\"model\", token=token)\n",
        "    print(temp)\n",
        "except RepositoryNotFoundError:\n",
        "    create_repo(repo_id, repo_type=\"model\", token=token)\n",
        "\n",
        "# Define the path to the directory and the files\n",
        "directory_path = \"/content/vision_transformer_model_progress/checkpoint-198\"\n",
        "files = [\"config.json\", \"model.safetensors\", \"optimizer.pt\", \"rng_state.pth\",\n",
        "         \"scheduler.pt\", \"trainer_state.json\", \"training_args.bin\"]\n",
        "\n",
        "# Upload each file to the repository\n",
        "for file_name in files:\n",
        "    file_path = os.path.join(directory_path, file_name)\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=file_path,\n",
        "        path_in_repo=file_name,\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"model\",  # Change this to \"dataset\" if you're uploading to a dataset repository\n",
        "        token=token,\n",
        "        commit_message=f\"Upload {file_name}\"\n",
        "    )\n",
        "\n",
        "print(\"Files uploaded successfully.\")"
      ],
      "metadata": {
        "id": "C84TZzw-r91i",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
