{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "570jXKiNpmNJ",
        "gFkqh6nGOp28",
        "mDJqa6LgkTtt",
        "0Q1m64xMkXWS",
        "Rw_ZFUmJkckO",
        "MCRQGdGTbbrQ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyllanesl/AI-EDGE-Project/blob/main/ClassiferTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download Appropriate Packages"
      ],
      "metadata": {
        "id": "7jnSB2hL8R_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow pillow\n",
        "!pip install mediapipe\n",
        "!pip install transformers datasets torch torchvision accelerate -U\n",
        "!pip install huggingface_hub\n",
        "!pip install pandas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mzBqHD6Bbh_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load dataset"
      ],
      "metadata": {
        "id": "570jXKiNpmNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "dataset = load_dataset('raulit04/Classifier-ASL2')"
      ],
      "metadata": {
        "id": "3D7vCkxEprU4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Preprocessing Functions\n",
        "\n"
      ],
      "metadata": {
        "id": "gFkqh6nGOp28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTFeatureExtractor\n",
        "\n",
        "# Load the feature extractor\n",
        "# feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('dyllanesl/ASL_Classifier')\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    # Convert images to RGB if they are not already\n",
        "    images = [image.convert(\"RGB\") for image in examples['image']]\n",
        "    inputs = feature_extractor(images, return_tensors=\"pt\")\n",
        "    inputs['label'] = examples['label']\n",
        "    return inputs\n",
        "\n",
        "# Apply the preprocessing function\n",
        "dataset = dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "pG67a-SOguID",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load pretrained model"
      ],
      "metadata": {
        "id": "mDJqa6LgkTtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTForImageClassification\n",
        "\n",
        "# Define the number of classes in your dataset\n",
        "num_labels = len(dataset['train'].unique('label'))\n",
        "\n",
        "# Load the pre-trained model with the correct number of output labels\n",
        "model = ViTForImageClassification.from_pretrained('dyllanesl/ASL_Classifier', num_labels=num_labels,ignore_mismatched_sizes=True, low_cpu_mem_usage=False)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zzflDirIkEyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define training arguments and trainer & Train Model"
      ],
      "metadata": {
        "id": "0Q1m64xMkXWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import torch\n",
        "# Define training arguments with output directory specified\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./vision_transformer_model_progress',  # output directory\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# Define data collator\n",
        "def data_collator(features):\n",
        "    # Convert pixel_values to tensors if they are not already\n",
        "    pixel_values = torch.stack([torch.tensor(f['pixel_values']) for f in features])  # Convert to tensors before stacking\n",
        "\n",
        "    # Handle string labels (assuming they are class names)\n",
        "    labels = torch.tensor([label_to_id[f['label']] for f in features])  # Map labels to integers\n",
        "\n",
        "    return {'pixel_values': pixel_values, 'labels': labels}\n",
        "\n",
        "# Create a mapping from labels to IDs\n",
        "label_list = dataset['train'].unique('label')  # Get unique labels\n",
        "label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
        "\n",
        "\n",
        "# Split the dataset into train and validation\n",
        "train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the final model\n",
        "trainer.save_model(\"./vision_transformer_model_progress\")"
      ],
      "metadata": {
        "id": "m-qnHkMFkavW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate the Model"
      ],
      "metadata": {
        "id": "Rw_ZFUmJkckO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)"
      ],
      "metadata": {
        "id": "Rt00t43Akk5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Model\n",
        "NOTE: During actual employment of project, make sure to integrate image taking in a loop"
      ],
      "metadata": {
        "id": "jE4jpO82T8G9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "            const div = document.createElement('div');\n",
        "            const capture = document.createElement('button');\n",
        "            capture.textContent = 'Capture';\n",
        "            div.appendChild(capture);\n",
        "\n",
        "            const video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "            document.body.appendChild(div);\n",
        "            div.appendChild(video);\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "\n",
        "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "            await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            div.remove();\n",
        "            return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "# Capture photo\n",
        "photo_filename = take_photo()"
      ],
      "metadata": {
        "id": "NN9XehcsDPiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "MARGIN = 10 # pixels\n",
        "FONT_SIZE = 1\n",
        "FONT_THICKNESS = 1\n",
        "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # Define HANDEDNESS_TEXT_COLOR\n",
        "\n",
        "# Function to draw landmarks on an image\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "    hand_landmarks_list = detection_result.multi_hand_landmarks\n",
        "    handedness_list = detection_result.multi_handedness\n",
        "    annotated_image = np.copy(rgb_image)\n",
        "\n",
        "    for idx in range(len(hand_landmarks_list)):\n",
        "        hand_landmarks = hand_landmarks_list[idx]\n",
        "        handedness = handedness_list[idx]\n",
        "\n",
        "        mp.solutions.drawing_utils.draw_landmarks(\n",
        "            annotated_image,\n",
        "            hand_landmarks,\n",
        "            mp.solutions.hands.HAND_CONNECTIONS,\n",
        "            mp.solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
        "            mp.solutions.drawing_styles.get_default_hand_connections_style()\n",
        "        )\n",
        "\n",
        "        height, width, _ = annotated_image.shape\n",
        "        x_coordinates = [landmark.x for landmark in hand_landmarks.landmark]\n",
        "        y_coordinates = [landmark.y for landmark in hand_landmarks.landmark]\n",
        "        text_x = int(min(x_coordinates) * width)\n",
        "        text_y = int(min(y_coordinates) * height) - MARGIN\n",
        "\n",
        "        cv2.putText(annotated_image, f\"{handedness.classification[0].label}\",\n",
        "                    (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
        "                    FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
        "\n",
        "    return annotated_image\n",
        "\n",
        "# Initialize MediaPipe Hands\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n",
        "\n",
        "# Load the captured image\n",
        "image = cv2.imread(photo_filename)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Process the image to detect hand landmarks\n",
        "results = hands.process(image_rgb)\n",
        "\n",
        "# Draw landmarks on the image\n",
        "if results.multi_hand_landmarks:\n",
        "    annotated_image = draw_landmarks_on_image(image_rgb, results)\n",
        "    annotated_image_bgr = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
        "    cv2.imwrite('annotated_photo.jpg', annotated_image_bgr)\n",
        "else:\n",
        "    annotated_image_bgr = image\n",
        "    print(\"No hand landmarks detected.\")\n"
      ],
      "metadata": {
        "id": "L9A5HvEvDP43"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "import torch\n",
        "\n",
        "# Load the feature extractor and model\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "model = ViTForImageClassification.from_pretrained('./vision_transformer_model_progress')\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "RDVWFMSIWfPm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Load and preprocess the image\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "    return inputs['pixel_values']\n",
        "\n",
        "image_path = '/content/annotated_photo.jpg'  # Replace with the path to your image\n",
        "pixel_values = preprocess_image(image_path)"
      ],
      "metadata": {
        "id": "exxXXJNjWjkg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(pixel_values)\n",
        "\n",
        "# Get predicted class label\n",
        "preds = outputs.logits.argmax(-1).item()\n",
        "\n",
        "# Map the prediction to the label\n",
        "id_to_label = {v: k for k, v in label_to_id.items()}  # Assuming label_to_id is defined as in your training script\n",
        "predicted_label = id_to_label[preds]\n",
        "\n",
        "# Ensure predicted_label is a string\n",
        "predicted_label_str = str(predicted_label)\n",
        "\n",
        "print(f'Predicted label: {predicted_label_str}')"
      ],
      "metadata": {
        "id": "7taUsHSDWnix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load CSV File and Create a Custom Dataset"
      ],
      "metadata": {
        "id": "AbCmGwNuO3aQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save Model Progress"
      ],
      "metadata": {
        "id": "MCRQGdGTbbrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "LUNunUhPbetD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "login(token = userdata.get('ASL_Token'))"
      ],
      "metadata": {
        "id": "N4YWXGHGrXYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, create_repo\n",
        "import os\n",
        "\n",
        "api = HfApi()\n",
        "repo_id = \"dyllanesl/ASL_Classifier\"  # Your repository name\n",
        "token = userdata.get('ASL_Token')  # Your Hugging Face token\n",
        "\n",
        "# Create the repository if it doesn't exist\n",
        "try:\n",
        "    temp = api.repo_info(repo_id, repo_type=\"model\", token=token)\n",
        "    print(temp)\n",
        "except RepositoryNotFoundError:\n",
        "    create_repo(repo_id, repo_type=\"model\", token=token)\n",
        "\n",
        "# Define the path to the directory and the files\n",
        "directory_path = \"/content/vision_transformer_model_progress/checkpoint-6\"\n",
        "files = [\"config.json\", \"model.safetensors\", \"optimizer.pt\", \"rng_state.pth\",\n",
        "         \"scheduler.pt\", \"trainer_state.json\", \"training_args.bin\"]\n",
        "\n",
        "# Upload each file to the repository\n",
        "for file_name in files:\n",
        "    file_path = os.path.join(directory_path, file_name)\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=file_path,\n",
        "        path_in_repo=file_name,\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"model\",  # Change this to \"dataset\" if you're uploading to a dataset repository\n",
        "        token=token,\n",
        "        commit_message=f\"Upload {file_name}\"\n",
        "    )\n",
        "\n",
        "print(\"Files uploaded successfully.\")"
      ],
      "metadata": {
        "id": "C84TZzw-r91i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}