{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fJlE_xehNFez"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyllanesl/AI-EDGE-Project/blob/main/ClassiferTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow pillow\n",
        "!pip install mediapipe\n",
        "!pip install transformers datasets torch torchvision accelerate -U\n",
        "!pip install huggingface_hub\n",
        "!pip install pandas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mzBqHD6Bbh_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Upload Dataset"
      ],
      "metadata": {
        "id": "fJlE_xehNFez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "ZwhHq2KCTEc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the dataset to a pandas DataFrame\n",
        "# Load the CSV file with labels\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "# Upload the CSV file\n",
        "uploaded = files.upload()\n",
        "\n",
        "csv_filename = list(uploaded.keys())[0]\n",
        "\n",
        "labels_df = pd.read_csv(\"ASL_Sheet.csv\")"
      ],
      "metadata": {
        "id": "MJD1GKY-NkA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pull images from Hugging Face\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"dyllanesl/Fruit_Images\")\n",
        "\n",
        "print(dataset)\n",
        "print(dataset[\"train\"][0])"
      ],
      "metadata": {
        "id": "pGG7tZbBSUTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test dataset\n",
        "#Import io and PIL\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "#Print the first item in the dataset\n",
        "image = dataset[\"train\"][0]['image']\n",
        "image\n",
        "\n",
        "#display\n",
        "labels_df['label']"
      ],
      "metadata": {
        "id": "QgPesbsMScru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Logging in\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "login(token = userdata.get('ASL_Token'))"
      ],
      "metadata": {
        "id": "mQptXyHZSoLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Dataset.from_dict({'image': dataset[\"train\"]['image'], 'label': labels_df['label']})\n",
        ")"
      ],
      "metadata": {
        "id": "x60QBh9WTAmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "#combine the images with the labels\n",
        "image_dataset = Dataset.from_dict({'image': dataset[\"train\"]['image'], 'label': labels_df['label']})\n",
        "#Display\n",
        "image_dataset"
      ],
      "metadata": {
        "id": "XnJzJHriTLWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Send it to huggingface\n",
        "image_dataset.push_to_hub(\"dyllanesl/Fruit_Dataset\")"
      ],
      "metadata": {
        "id": "WX_tkMzJTTu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load dataset"
      ],
      "metadata": {
        "id": "570jXKiNpmNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "dataset = load_dataset('raulit04/Classifier-ASL2')"
      ],
      "metadata": {
        "id": "3D7vCkxEprU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Preprocessing Functions\n",
        "\n"
      ],
      "metadata": {
        "id": "gFkqh6nGOp28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTFeatureExtractor\n",
        "\n",
        "# Load the feature extractor\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    # Convert images to RGB if they are not already\n",
        "    images = [image.convert(\"RGB\") for image in examples['image']]\n",
        "    inputs = feature_extractor(images, return_tensors=\"pt\")\n",
        "    inputs['label'] = examples['label']\n",
        "    return inputs\n",
        "\n",
        "# Apply the preprocessing function\n",
        "dataset = dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "id": "pG67a-SOguID",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load pretrained model"
      ],
      "metadata": {
        "id": "mDJqa6LgkTtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTForImageClassification\n",
        "\n",
        "# Define the number of classes in your dataset\n",
        "num_labels = len(dataset['train'].unique('label'))\n",
        "\n",
        "# Load the pre-trained model with the correct number of output labels\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=num_labels,ignore_mismatched_sizes=True, low_cpu_mem_usage=False)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zzflDirIkEyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a97441ce-3b37-40d3-f2ed-d60eebc3bc05"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([24]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([24, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define training arguments and trainer"
      ],
      "metadata": {
        "id": "0Q1m64xMkXWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import torch\n",
        "# Define training arguments with output directory specified\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./vision_transformer_model_progress',  # output directory\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# Define data collator\n",
        "def data_collator(features):\n",
        "    # Convert pixel_values to tensors if they are not already\n",
        "    pixel_values = torch.stack([torch.tensor(f['pixel_values']) for f in features])  # Convert to tensors before stacking\n",
        "\n",
        "    # Handle string labels (assuming they are class names)\n",
        "    labels = torch.tensor([label_to_id[f['label']] for f in features])  # Map labels to integers\n",
        "\n",
        "    return {'pixel_values': pixel_values, 'labels': labels}\n",
        "\n",
        "# Create a mapping from labels to IDs\n",
        "label_list = dataset['train'].unique('label')  # Get unique labels\n",
        "label_to_id = {label: idx for idx, label in enumerate(label_list)}\n",
        "\n",
        "\n",
        "# Split the dataset into train and validation\n",
        "train_test_split = dataset['train'].train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the final model\n",
        "trainer.save_model(\"./vision_transformer_model_progress\")"
      ],
      "metadata": {
        "id": "m-qnHkMFkavW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "908cfb00-965c-4f6d-b605-4c0082835625"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 01:40, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate the Model"
      ],
      "metadata": {
        "id": "Rw_ZFUmJkckO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)"
      ],
      "metadata": {
        "id": "Rt00t43Akk5C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "821e98ed-5931-4d48-a4dc-4119fcde9830"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1/1 : < :]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 3.4070675373077393, 'eval_runtime': 4.958, 'eval_samples_per_second': 1.008, 'eval_steps_per_second': 0.202, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test Model"
      ],
      "metadata": {
        "id": "jE4jpO82T8G9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "import torch\n",
        "\n",
        "# Load the feature extractor and model\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "model = ViTForImageClassification.from_pretrained('./vision_transformer_model_progress')\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "RDVWFMSIWfPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Load and preprocess the image\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "    return inputs['pixel_values']\n",
        "\n",
        "image_path = '/content/image.jpg'  # Replace with the path to your image\n",
        "pixel_values = preprocess_image(image_path)"
      ],
      "metadata": {
        "id": "exxXXJNjWjkg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(pixel_values)\n",
        "\n",
        "# Get predicted class label\n",
        "preds = outputs.logits.argmax(-1).item()\n",
        "\n",
        "# Map the prediction to the label\n",
        "id_to_label = {v: k for k, v in label_to_id.items()}  # Assuming label_to_id is defined as in your training script\n",
        "predicted_label = id_to_label[preds]\n",
        "\n",
        "# Ensure predicted_label is a string\n",
        "predicted_label_str = str(predicted_label)\n",
        "\n",
        "print(f'Predicted label: {predicted_label_str}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7taUsHSDWnix",
        "outputId": "eaa05edf-3ca0-4038-af38-7e91d7563eb8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LOad CSV File and Create a Custom Dataset"
      ],
      "metadata": {
        "id": "AbCmGwNuO3aQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Load the uploaded image\n",
        "image_path = list(uploaded.keys())[0]\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Classify the image using the pipeline\n",
        "results = pipe(image)\n",
        "\n",
        "# Print the results\n",
        "for result in results:\n",
        "    print(f\"Label: {result['label']}, Score: {result['score']:.4f}\")\n"
      ],
      "metadata": {
        "id": "220YeeYjh6ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "        async function takePhoto(quality) {\n",
        "            const div = document.createElement('div');\n",
        "            const capture = document.createElement('button');\n",
        "            capture.textContent = 'Capture';\n",
        "            div.appendChild(capture);\n",
        "\n",
        "            const video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "            document.body.appendChild(div);\n",
        "            div.appendChild(video);\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "\n",
        "            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "            await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            div.remove();\n",
        "            return canvas.toDataURL('image/jpeg', quality);\n",
        "        }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "# Capture photo\n",
        "photo_filename = take_photo()\n"
      ],
      "metadata": {
        "id": "LA1cQ9FpiWLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "\n",
        "MARGIN = 10 # pixels\n",
        "FONT_SIZE = 1\n",
        "FONT_THICKNESS = 1\n",
        "HANDEDNESS_TEXT_COLOR = (88, 205, 54) # Define HANDEDNESS_TEXT_COLOR\n",
        "\n",
        "# Function to draw landmarks on an image\n",
        "def draw_landmarks_on_image(rgb_image, detection_result):\n",
        "    hand_landmarks_list = detection_result.multi_hand_landmarks\n",
        "    handedness_list = detection_result.multi_handedness\n",
        "    annotated_image = np.copy(rgb_image)\n",
        "\n",
        "    for idx in range(len(hand_landmarks_list)):\n",
        "        hand_landmarks = hand_landmarks_list[idx]\n",
        "        handedness = handedness_list[idx]\n",
        "\n",
        "        mp.solutions.drawing_utils.draw_landmarks(\n",
        "            annotated_image,\n",
        "            hand_landmarks,\n",
        "            mp.solutions.hands.HAND_CONNECTIONS,\n",
        "            mp.solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
        "            mp.solutions.drawing_styles.get_default_hand_connections_style()\n",
        "        )\n",
        "\n",
        "        height, width, _ = annotated_image.shape\n",
        "        x_coordinates = [landmark.x for landmark in hand_landmarks.landmark]\n",
        "        y_coordinates = [landmark.y for landmark in hand_landmarks.landmark]\n",
        "        text_x = int(min(x_coordinates) * width)\n",
        "        text_y = int(min(y_coordinates) * height) - MARGIN\n",
        "\n",
        "        cv2.putText(annotated_image, f\"{handedness.classification[0].label}\",\n",
        "                    (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
        "                    FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
        "\n",
        "    return annotated_image\n",
        "\n",
        "# Initialize MediaPipe Hands\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)\n",
        "\n",
        "# Load the captured image\n",
        "image = cv2.imread(photo_filename)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Process the image to detect hand landmarks\n",
        "results = hands.process(image_rgb)\n",
        "\n",
        "# Draw landmarks on the image\n",
        "if results.multi_hand_landmarks:\n",
        "    annotated_image = draw_landmarks_on_image(image_rgb, results)\n",
        "    annotated_image_bgr = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
        "    cv2.imwrite('annotated_photo.jpg', annotated_image_bgr)\n",
        "else:\n",
        "    annotated_image_bgr = image\n",
        "    print(\"No hand landmarks detected.\")\n"
      ],
      "metadata": {
        "id": "Q6-A66MxigsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from PIL import Image\n",
        "\n",
        "# Initialize the Hugging Face image classification pipeline\n",
        "pipe = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\n",
        "\n",
        "# Load the annotated image\n",
        "annotated_image_pil = Image.open('annotated_photo.jpg')\n",
        "\n",
        "# Classify the image using the Hugging Face pipeline\n",
        "classification_results = pipe(annotated_image_pil)\n",
        "\n",
        "# Print the classification results\n",
        "for result in classification_results:\n",
        "    print(f\"Label: {result['label']}, Score: {result['score']:.4f}\")\n"
      ],
      "metadata": {
        "id": "jgB5lY0FiiG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save Model Progress"
      ],
      "metadata": {
        "id": "MCRQGdGTbbrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "LUNunUhPbetD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}